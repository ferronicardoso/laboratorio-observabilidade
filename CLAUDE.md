# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

---

## üî¨ Sobre o Projeto

Este √© um **laborat√≥rio educacional de observabilidade** que demonstra conceitos modernos de monitoramento, logging e m√©tricas usando a Stack Grafana (Prometheus, Loki, Alloy, Grafana) com aplica√ß√µes em m√∫ltiplas linguagens (.NET, Python, Java, TypeScript).

**Stack:**
- **Observabilidade**: Grafana, Prometheus, Loki, Grafana Tempo, Grafana Alloy, Node Exporter, Windows Exporter, PostgreSQL Exporter, MSSQL Exporter, MySQL Exporter
- **Aplica√ß√µes**: .NET API, Python FastAPI, Java Spring Boot, Next.js, Angular, Nginx
- **Bancos de Dados**: PostgreSQL (Python API), SQL Server 2019 (.NET API), MySQL (Java API)
- **Infraestrutura**: Docker + Docker Compose

---

## üöÄ Comandos Essenciais

### Executar o Projeto

```bash
# Subir toda a stack
docker compose up -d

# Verificar status dos containers
docker compose ps

# Ver logs de um servi√ßo espec√≠fico
docker logs <container-name>

# Parar a stack
docker compose down

# Rebuild de um servi√ßo espec√≠fico
docker compose up -d --build <service-name>

# Rebuild completo
docker compose down && docker compose up -d --build
```

### Acessar Interfaces

- **Grafana**: http://localhost:3000 (admin/admin)
- **Prometheus**: http://localhost:9090
- **Tempo**: http://localhost:3200 (API)
- **pgAdmin**: http://localhost:5050 (gerenciamento PostgreSQL)
- **Next.js**: http://localhost:3001
- **Angular**: http://localhost:4200
- **.NET API**: http://localhost:5000
- **Python API**: http://localhost:8001
- **Java API**: http://localhost:8002
- **Nginx**: http://localhost:8080
- **PostgreSQL**: localhost:5432 (labuser/labpass) - Python API
- **SQL Server**: localhost:1433 (sa/YourStrong!Passw0rd) - .NET API
- **MySQL**: localhost:3306 (labuser/labpass) - Java API
- **PostgreSQL Exporter**: http://localhost:9187/metrics
- **MSSQL Exporter**: http://localhost:4000/metrics
- **MySQL Exporter**: http://localhost:9104/metrics

### Testar M√©tricas

```bash
# Verificar targets no Prometheus
curl -s http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | {job: .labels.job, health: .health}'

# Ver m√©tricas de uma aplica√ß√£o
curl http://localhost:5000/metrics  # .NET
curl http://localhost:8001/metrics  # Python
curl http://localhost:8002/actuator/prometheus  # Java

# Ver m√©tricas dos bancos de dados
curl http://localhost:9187/metrics  # PostgreSQL
curl http://localhost:4000/metrics  # SQL Server
curl http://localhost:9104/metrics  # MySQL

# Gerar tr√°fego para testes
for i in {1..50}; do curl -s http://localhost:5000/weatherforecast > /dev/null; done
```

### Queries √öteis

**PromQL (Prometheus):**
```promql
# Requisi√ß√µes por segundo
rate(http_server_request_duration_seconds_count{job="dotnet-api"}[1m])

# Uso de CPU do Windows
100 - (avg(rate(windows_cpu_time_total{mode="idle",job="node-exporter-windows"}[2m])) * 100)

# Uso de CPU do Linux
100 - (avg(irate(node_cpu_seconds_total{mode="idle",job="node-exporter-linux"}[5m])) * 100)
```

**LogQL (Loki):**
```logql
# Logs do Nginx
{job="nginx"}

# Logs de uma aplica√ß√£o espec√≠fica
{container="dotnet-api"}

# Taxa de logs
rate({container="python-api"}[1m])
```

**TraceQL (Tempo):**
```traceql
# Todos os traces de um servi√ßo
{ resource.service.name="dotnet-api" }

# Traces com queries SQL
{ resource.service.name="dotnet-api" && span.db.statement != nil }

# Traces com dura√ß√£o > 100ms
{ resource.service.name="dotnet-api" && duration > 100ms }

# Traces com erro
{ resource.service.name="dotnet-api" && status = error }
```

---

## üîç Distributed Tracing (Fase 2)

### Grafana Tempo v2.9.1

**‚ö†Ô∏è IMPORTANTE - Bug na vers√£o 2.10.0:**
A vers√£o 2.10.0 do Tempo tem um bug conhecido onde o m√≥dulo `ingester` n√£o √© inicializado em modo monolithic (single-binary), causando o erro "InstancesCount <= 0". Use a vers√£o **2.9.1** ou anteriores (2.6.0, 2.7.x, 2.8.x, 2.9.x) at√© que o bug seja corrigido.

**Vers√µes testadas e funcionais:**
- ‚úÖ v2.6.0 - funciona
- ‚úÖ v2.8.2 - funciona
- ‚úÖ v2.9.1 - funciona (recomendada)
- ‚ùå v2.10.0 - ingester n√£o inicializa

### Componentes de Tracing

**Stack de Tracing:**
- **Tempo 2.9.1**: Backend para armazenamento de traces
- **OpenTelemetry**: Instrumenta√ß√£o da API .NET
- **Grafana Alloy**: Coletor de traces (OTLP receiver)
- **PostgreSQL 18-alpine**: Banco de dados com 1000 produtos para queries realistas

**Fluxo de Dados:**
1. API .NET gera traces com OpenTelemetry SDK
2. Traces incluem spans HTTP, Entity Framework Core (SQL queries)
3. Alloy recebe traces via OTLP (portas 4317 gRPC / 4318 HTTP)
4. Alloy encaminha para Tempo
5. Tempo armazena traces e gera m√©tricas (service graphs, span metrics)
6. M√©tricas s√£o enviadas ao Prometheus via remote_write
7. Grafana consulta traces no Tempo e visualiza Service Graph

### Configura√ß√£o do Tempo

**Config m√≠nima para modo monolithic (`tempo-config.yml`):**
```yaml
stream_over_http_enabled: true

server:
  http_listen_port: 3200

distributor:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317  # IMPORTANTE: 0.0.0.0, n√£o 127.0.0.1
        http:
          endpoint: 0.0.0.0:4318

ingester:
  lifecycler:
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
  max_block_duration: 5m

metrics_generator:
  registry:
    external_labels:
      source: tempo
      cluster: lab-observabilidade
  storage:
    path: /var/tempo/generator/wal
    remote_write:
      - url: http://prometheus:9090/api/v1/write
        send_exemplars: true

storage:
  trace:
    backend: local
    local:
      path: /var/tempo/blocks
    wal:
      path: /var/tempo/wal

overrides:
  defaults:
    metrics_generator:
      processors: [service-graphs, span-metrics]
```

**Configura√ß√£o do Prometheus:**
- Adicionar flag `--web.enable-remote-write-receiver` para aceitar m√©tricas do Tempo
- Tempo envia m√©tricas de service graphs e span metrics automaticamente

### Visualizar Traces no Grafana

**1. Grafana Explore:**
- URL: http://localhost:3000/explore
- Selecionar datasource "Tempo"

**2. Search (interface visual):**
- Service Name: `dotnet-api`
- Span Name: filtros opcionais
- Tags: `http.method`, `http.status_code`, etc.

**3. TraceQL (queries avan√ßadas):**
```traceql
# ‚ö†Ô∏è IMPORTANTE: usar resource.service.name, N√ÉO service.name
{ resource.service.name="dotnet-api" }
{ resource.service.name="dotnet-api" && span.db.statement != nil }
{ resource.service.name="dotnet-api" && duration > 100ms }
```

**4. Service Graph:**
- Visualiza√ß√£o do fluxo de requisi√ß√µes entre servi√ßos
- Mostra taxa de requisi√ß√µes, lat√™ncia e erros
- Requer m√©tricas do metrics_generator no Prometheus

**5. Correla√ß√£o com Logs:**
- Clicar em um span no trace
- Grafana busca logs correlacionados automaticamente via tags

### Instrumenta√ß√£o da API .NET

**Pacotes necess√°rios:**
```xml
<PackageReference Include="OpenTelemetry.Exporter.OpenTelemetryProtocol" Version="1.10.0" />
<PackageReference Include="OpenTelemetry.Instrumentation.AspNetCore" Version="1.10.0" />
<PackageReference Include="OpenTelemetry.Instrumentation.Http" Version="1.10.0" />
<PackageReference Include="OpenTelemetry.Instrumentation.EntityFrameworkCore" Version="1.0.0-beta.14" />
```

**Configura√ß√£o (`Program.cs`):**
```csharp
builder.Services.AddOpenTelemetry()
    .WithTracing(tracing =>
    {
        tracing
            .SetResourceBuilder(ResourceBuilder.CreateDefault()
                .AddService("dotnet-api", serviceVersion: "1.0.0"))
            .AddAspNetCoreInstrumentation(options =>
            {
                options.RecordException = true;
                options.EnrichWithHttpRequest = (activity, request) =>
                {
                    activity.SetTag("http.request.method", request.Method);
                    activity.SetTag("http.request.path", request.Path);
                };
            })
            .AddHttpClientInstrumentation()
            .AddEntityFrameworkCoreInstrumentation(options =>
            {
                options.SetDbStatementForText = true;
                options.EnrichWithIDbCommand = (activity, command) =>
                {
                    activity.SetTag("db.query", command.CommandText);
                };
            })
            .AddOtlpExporter(options =>
            {
                options.Endpoint = new Uri("http://alloy:4317");
                options.Protocol = OtlpExportProtocol.Grpc;
            });
    });
```

**Spans gerados automaticamente:**
- ‚úÖ HTTP requests (ASP.NET Core)
- ‚úÖ SQL queries (Entity Framework Core)
- ‚úÖ HTTP client calls
- ‚úÖ Exce√ß√µes (quando configurado)

**Atributos √∫teis nos traces:**
- `http.method`, `http.route`, `http.status_code`
- `db.statement` - SQL query completa
- `db.system`, `db.name` - informa√ß√µes do banco
- Dura√ß√£o de cada span em microssegundos

### Gerar Tr√°fego para Traces

```bash
# GET produtos (pagina√ß√£o + SQL queries)
for i in {1..10}; do
  curl -s "http://localhost:5000/api/products?page=$((RANDOM % 10 + 1))&pageSize=5" > /dev/null
  sleep 0.2
done

# GET por ID (queries SQL espec√≠ficas)
for i in {1..10}; do
  curl -s "http://localhost:5000/api/products/$((RANDOM % 1000 + 1))" > /dev/null
  sleep 0.2
done

# Count
for i in {1..5}; do
  curl -s "http://localhost:5000/api/products/count" > /dev/null
  sleep 0.2
done
```

### Troubleshooting Traces

**Traces n√£o aparecem no Grafana:**
1. Verificar se Tempo est√° rodando: `docker logs tempo | grep "starting module=ingester"`
2. Verificar se Alloy est√° encaminhando: `docker logs alloy | grep tempo`
3. Verificar endpoints OTLP: devem ser `0.0.0.0:4317` e n√£o `127.0.0.1`
4. Gerar tr√°fego na API para criar traces

**Service Graph vazio:**
1. Verificar se Prometheus aceita remote write: flag `--web.enable-remote-write-receiver`
2. Verificar se metrics_generator tem `remote_write` configurado
3. Aguardar 1-2 minutos ap√≥s gerar tr√°fego
4. Verificar m√©tricas no Prometheus: `curl http://localhost:9090/api/v1/label/__name__/values | grep traces_service_graph`

**Erro "InstancesCount <= 0":**
- Vers√£o 2.10.0 do Tempo est√° com bug
- Usar vers√£o 2.9.1 ou anterior

---

## üóÑÔ∏è Database Monitoring (PostgreSQL)

### PostgreSQL Exporter

O projeto usa o **PostgreSQL Exporter** oficial para coletar m√©tricas do banco de dados e envi√°-las ao Prometheus.

**Componentes:**
- **postgres-exporter**: Coleta m√©tricas do PostgreSQL
- **Prometheus**: Armazena m√©tricas via scrape (porta 9187)
- **Grafana**: Dashboard com visualiza√ß√µes

**Configura√ß√£o:**
```yaml
# docker-compose.yml
postgres-exporter:
  image: quay.io/prometheuscommunity/postgres-exporter:latest
  environment:
    DATA_SOURCE_NAME: "postgresql://labuser:labpass@postgres:5432/observability_lab?sslmode=disable"
  ports:
    - "9187:9187"
```

### M√©tricas Dispon√≠veis

**Status e Disponibilidade:**
- `pg_up` - Status do PostgreSQL (1 = UP, 0 = DOWN)

**Conex√µes:**
- `pg_stat_activity_count` - Conex√µes ativas, idle, etc.
- `pg_stat_database_numbackends` - N√∫mero de backends conectados

**Performance:**
- `pg_stat_database_blks_hit` - Cache hits (blocos lidos do cache)
- `pg_stat_database_blks_read` - Cache misses (blocos lidos do disco)
- Cache Hit Ratio = `blks_hit / (blks_hit + blks_read) * 100`

**Transa√ß√µes:**
- `pg_stat_database_xact_commit` - Transa√ß√µes commitadas
- `pg_stat_database_xact_rollback` - Transa√ß√µes com rollback

**Opera√ß√µes de Dados:**
- `pg_stat_database_tup_inserted` - Linhas inseridas
- `pg_stat_database_tup_updated` - Linhas atualizadas
- `pg_stat_database_tup_deleted` - Linhas deletadas

**Armazenamento:**
- `pg_database_size_bytes` - Tamanho do banco em bytes

**Locks e Deadlocks:**
- `pg_locks_count` - N√∫mero de locks por tipo
- `pg_stat_database_deadlocks` - Deadlocks detectados

### Queries PromQL √öteis

```promql
# Cache hit ratio (deve ser > 95%)
sum(pg_stat_database_blks_hit{datname="observability_lab"}) /
(sum(pg_stat_database_blks_hit{datname="observability_lab"}) +
 sum(pg_stat_database_blks_read{datname="observability_lab"})) * 100

# Transa√ß√µes por segundo
rate(pg_stat_database_xact_commit{datname="observability_lab"}[1m])

# Conex√µes ativas
sum(pg_stat_activity_count{state="active"})

# Taxa de inserts/updates/deletes
rate(pg_stat_database_tup_inserted{datname="observability_lab"}[1m])
rate(pg_stat_database_tup_updated{datname="observability_lab"}[1m])
rate(pg_stat_database_tup_deleted{datname="observability_lab"}[1m])
```

### Dashboard do Grafana

O dashboard **PostgreSQL - Database Monitoring** (`postgresql.json`) cont√©m:

**Primeira linha (Stats/Gauges):**
- Status do PostgreSQL (UP/DOWN)
- Conex√µes ativas
- Conex√µes idle
- Cache hit ratio (gauge 0-100%)
- Tamanho do banco

**Time Series:**
- Conex√µes ao longo do tempo (total, active, idle)
- Transa√ß√µes por segundo (commits vs rollbacks)
- Backends conectados
- Locks por tipo
- Opera√ß√µes de tuplas (inserts, updates, deletes)
- Deadlocks

### Verificar M√©tricas

```bash
# Verificar se exporter est√° UP no Prometheus
curl -s http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | select(.labels.job=="postgres")'

# Ver m√©tricas do PostgreSQL
curl http://localhost:9187/metrics

# Ver m√©tricas espec√≠ficas
curl -s http://localhost:9187/metrics | grep pg_up
curl -s http://localhost:9187/metrics | grep pg_stat_database_blks
```

### Gerar Tr√°fego no Banco

```bash
# Gerar queries para observar m√©tricas
for i in {1..20}; do
  curl -s "http://localhost:5000/api/products?page=$((RANDOM % 10 + 1))&pageSize=10" > /dev/null
  sleep 0.5
done

# Ver conex√µes ativas no Prometheus
curl -s 'http://localhost:9090/api/v1/query?query=sum(pg_stat_activity_count{state="active"})' | jq '.data.result[0].value[1]'

# Ver cache hit ratio no Prometheus
curl -s 'http://localhost:9090/api/v1/query?query=sum(pg_stat_database_blks_hit{datname="observability_lab"})/(sum(pg_stat_database_blks_hit{datname="observability_lab"})+sum(pg_stat_database_blks_read{datname="observability_lab"}))*100' | jq '.data.result[0].value[1]'
```

### Troubleshooting Database Monitoring

**Exporter n√£o aparece no Prometheus:**
1. Verificar se container est√° rodando: `docker ps | grep postgres-exporter`
2. Verificar logs: `docker logs postgres-exporter`
3. Verificar configura√ß√£o: `DATA_SOURCE_NAME` deve estar correto
4. Testar conex√£o: `docker exec postgres-exporter wget -qO- localhost:9187/metrics`

**M√©tricas zeradas ou vazias:**
1. Verificar se PostgreSQL est√° UP: `docker logs postgres`
2. Verificar conex√£o do exporter ao banco: `docker logs postgres-exporter | grep -i error`
3. Gerar tr√°fego na API para criar conex√µes/queries

**Dashboard vazio:**
1. Verificar se job `postgres` est√° no Prometheus: `http://localhost:9090/targets`
2. Verificar queries no Grafana (usar Query Inspector)
3. Ajustar time range (usar √∫ltimos 30 minutos)

---

## üóÑÔ∏è Database Monitoring (SQL Server)

### SQL Server Exporter

O projeto usa o **MSSQL Exporter** (awaragi/prometheus-mssql-exporter) para coletar m√©tricas do SQL Server.

**Componentes:**
- **mssql-exporter**: Coleta m√©tricas do SQL Server
- **Prometheus**: Armazena m√©tricas via scrape (porta 4000)
- **Grafana**: Dashboard com visualiza√ß√µes

**Configura√ß√£o:**
```yaml
# docker-compose.yml
mssqlserver:
  image: mcr.microsoft.com/mssql/server:2019-latest
  environment:
    SA_PASSWORD: "YourStrong!Passw0rd"
    ACCEPT_EULA: "Y"
  ports:
    - "1433:1433"

mssql-exporter:
  image: awaragi/prometheus-mssql-exporter:latest
  environment:
    SERVER: "mssqlserver"
    USERNAME: "sa"
    PASSWORD: "YourStrong!Passw0rd"
  ports:
    - "4000:4000"
```

### M√©tricas Dispon√≠veis

**Status:**
- `mssql_up` - Status do SQL Server (1 = UP, 0 = DOWN)

**Conex√µes:**
- `mssql_connections` - N√∫mero de conex√µes ativas

**Performance:**
- `mssql_buffer_cache_hit_ratio` - Buffer cache hit ratio (%)
- `mssql_page_life_expectancy` - Page Life Expectancy (segundos)

**Requisi√ß√µes:**
- `mssql_batch_requests` - Batch requests por segundo
- `mssql_sql_compilations` - SQL compilations por segundo
- `mssql_sql_recompilations` - SQL recompilations por segundo

**Mem√≥ria:**
- `mssql_server_total_server_memory_bytes` - Mem√≥ria total usada
- `mssql_os_sys_memory` - Mem√≥ria total do sistema

**Locks e Bloqueios:**
- `mssql_lock_waits` - Lock waits
- `mssql_deadlocks` - Deadlocks detectados

### Queries PromQL √öteis

```promql
# Buffer cache hit ratio (deve ser > 80%)
mssql_buffer_cache_hit_ratio{job="mssql"}

# Batch requests por segundo
rate(mssql_batch_requests{job="mssql"}[1m])

# Conex√µes ativas
mssql_connections{job="mssql"}

# Uso de mem√≥ria (%)
mssql_server_total_server_memory_bytes{job="mssql"} / mssql_os_sys_memory{job="mssql"} * 100

# Deadlocks
mssql_deadlocks{job="mssql"}
```

### Dashboard do Grafana

O dashboard **SQL Server - Database Monitoring** (`mssql.json`) cont√©m:

**Primeira linha (Stats/Gauges):**
- Status do SQL Server (UP/DOWN)
- Conex√µes ativas
- Buffer cache hit ratio (gauge 0-100%)
- Uso de mem√≥ria (%)
- Batch requests/s

**Time Series:**
- Conex√µes ao longo do tempo
- Batch requests e compila√ß√µes SQL
- Uso de mem√≥ria
- Deadlocks

### Verificar M√©tricas

```bash
# Verificar se exporter est√° UP no Prometheus
curl -s http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | select(.labels.job=="mssql")'

# Ver m√©tricas do SQL Server
curl http://localhost:4000/metrics

# Ver m√©tricas espec√≠ficas
curl -s http://localhost:4000/metrics | grep mssql_up
curl -s http://localhost:4000/metrics | grep mssql_buffer_cache_hit_ratio
```

### Testar com k6

```bash
# Script k6 para validar m√©tricas do SQL Server
k6 run tests/k6/test-mssql-metrics.js
```

**O script k6 valida:**
- ‚úÖ Status do SQL Server Exporter
- ‚úÖ Conex√µes
- ‚úÖ Buffer Cache Hit Ratio (threshold > 80%)
- ‚úÖ Batch Requests
- ‚úÖ Uso de Mem√≥ria
- ‚úÖ Deadlocks

### Troubleshooting SQL Server Monitoring

**Exporter n√£o aparece no Prometheus:**
1. Verificar se container est√° rodando: `docker ps | grep mssql-exporter`
2. Verificar logs: `docker logs mssql-exporter`
3. Verificar conex√£o: `docker logs mssql-exporter | grep -i error`
4. Testar endpoint: `curl http://localhost:4000/metrics`

**M√©tricas zeradas ou vazias:**
1. Verificar se SQL Server est√° UP: `docker logs mssqlserver`
2. Verificar credenciais do exporter (SA_PASSWORD)
3. Aguardar SQL Server inicializar completamente (~30s)

**Dashboard vazio:**
1. Verificar se job `mssql` est√° no Prometheus: `http://localhost:9090/targets`
2. Verificar queries no Grafana (usar Query Inspector)
3. Ajustar time range (usar √∫ltimos 30 minutos)

---

## üóÑÔ∏è Database Monitoring (MySQL)

### MySQL Exporter

O projeto usa o **MySQL Exporter** oficial (Prometheus community) para coletar m√©tricas do MySQL.

**Componentes:**
- **mysql-exporter**: Coleta m√©tricas do MySQL
- **Prometheus**: Armazena m√©tricas via scrape (porta 9104)
- **Grafana**: Dashboard com visualiza√ß√µes

**Configura√ß√£o:**
```yaml
# docker-compose.yml
mysql:
  image: mysql:latest
  environment:
    MYSQL_ROOT_PASSWORD: "rootpass"
    MYSQL_DATABASE: "observability_lab"
    MYSQL_USER: "labuser"
    MYSQL_PASSWORD: "labpass"
  ports:
    - "3306:3306"

mysql-exporter:
  image: prom/mysqld-exporter:latest
  command:
    - "--config.my-cnf=/etc/.my.cnf"
  volumes:
    - ./observability/mysql-exporter/.my.cnf:/etc/.my.cnf:ro
  ports:
    - "9104:9104"
```

**Arquivo de configura√ß√£o (`observability/mysql-exporter/.my.cnf`):**
```ini
[client]
user=labuser
password=labpass
host=mysql
port=3306
```

### M√©tricas Dispon√≠veis

**Status:**
- `mysql_up` - Status do MySQL (1 = UP, 0 = DOWN)

**Conex√µes:**
- `mysql_global_status_threads_connected` - Threads conectadas
- `mysql_global_status_threads_running` - Threads rodando

**Performance:**
- `mysql_global_status_queries` - Total de queries
- `mysql_global_status_questions` - Total de questions
- `mysql_global_status_slow_queries` - Slow queries

**Network:**
- `mysql_global_status_bytes_received` - Bytes recebidos
- `mysql_global_status_bytes_sent` - Bytes enviados

**InnoDB:**
- `mysql_global_status_innodb_data_written` - Data written
- `mysql_global_status_innodb_data_read` - Data read

**Uptime:**
- `mysql_global_status_uptime` - Uptime em segundos

### Queries PromQL √öteis

```promql
# Queries por segundo
rate(mysql_global_status_queries{job="mysql"}[1m])

# Conex√µes ativas
mysql_global_status_threads_connected{job="mysql"}

# Threads rodando
mysql_global_status_threads_running{job="mysql"}

# Slow queries
mysql_global_status_slow_queries{job="mysql"}

# Network throughput (bytes/s)
rate(mysql_global_status_bytes_received{job="mysql"}[1m])
rate(mysql_global_status_bytes_sent{job="mysql"}[1m])
```

### Dashboard do Grafana

O dashboard **MySQL - Database Monitoring** (`mysql.json`) cont√©m:

**Primeira linha (Stats):**
- Status do MySQL (UP/DOWN)
- Conex√µes ativas
- Queries/s
- InnoDB Data Written
- Uptime

**Time Series:**
- Conex√µes e threads ao longo do tempo
- Queries por segundo
- Network traffic
- Slow queries

### Verificar M√©tricas

```bash
# Verificar se exporter est√° UP no Prometheus
curl -s http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | select(.labels.job=="mysql")'

# Ver m√©tricas do MySQL
curl http://localhost:9104/metrics

# Ver m√©tricas espec√≠ficas
curl -s http://localhost:9104/metrics | grep mysql_up
curl -s http://localhost:9104/metrics | grep mysql_global_status_threads_connected
```

### Testar com k6

```bash
# Script k6 para validar m√©tricas do MySQL
k6 run tests/k6/test-mysql-metrics.js
```

**O script k6 valida:**
- ‚úÖ Status do MySQL Exporter
- ‚úÖ Conex√µes e threads
- ‚úÖ Queries por segundo
- ‚úÖ Slow queries
- ‚úÖ Uptime

### Troubleshooting MySQL Monitoring

**Exporter n√£o aparece no Prometheus:**
1. Verificar se container est√° rodando: `docker ps | grep mysql-exporter`
2. Verificar logs: `docker logs mysql-exporter`
3. Verificar conex√£o: `docker logs mysql-exporter | grep -i error`
4. Testar endpoint: `curl http://localhost:9104/metrics`

**M√©tricas zeradas ou vazias:**
1. Verificar se MySQL est√° UP: `docker logs mysql`
2. Verificar credenciais do exporter (DATA_SOURCE_NAME)
3. Aguardar MySQL inicializar completamente (~20s)

**Dashboard vazio:**
1. Verificar se job `mysql` est√° no Prometheus: `http://localhost:9090/targets`
2. Verificar queries no Grafana (usar Query Inspector)
3. Ajustar time range (usar √∫ltimos 30 minutos)

---

## üéØ Service Level Objectives (SLOs)

### O que s√£o SLOs?

**SLIs (Service Level Indicators)** s√£o m√©tricas que medem um aspecto espec√≠fico do n√≠vel de servi√ßo (ex: lat√™ncia, disponibilidade).

**SLOs (Service Level Objectives)** s√£o targets para os SLIs (ex: "99.9% de disponibilidade").

**Error Budget** √© a quantidade de "erro" permitida dentro do SLO (ex: se SLO √© 99.9%, o error budget √© 0.1%).

**Burn Rate** √© a velocidade de consumo do error budget (ex: burn rate de 5x significa consumindo 5x mais r√°pido que o esperado).

### SLOs Definidos

Todos os SLOs s√£o medidos em janela de **30 dias**:

| API | SLI | Target | Descri√ß√£o |
|-----|-----|--------|-----------|
| .NET API | Availability | ‚â• 99.9% | % de requisi√ß√µes 2xx |
| .NET API | Latency P95 | < 200ms | Percentil 95 do tempo de resposta |
| .NET API | Error Rate | < 0.1% | % de requisi√ß√µes 5xx |
| Python API | Availability | ‚â• 99.9% | % de requisi√ß√µes 2xx |
| Python API | Latency P95 | < 200ms | Percentil 95 do tempo de resposta |
| Python API | Error Rate | < 0.1% | % de requisi√ß√µes 5xx |
| Java API | Availability | ‚â• 99.9% | % de requisi√ß√µes 2xx |
| Java API | Latency P95 | < 200ms | Percentil 95 do tempo de resposta |
| Java API | Error Rate | < 0.1% | % de requisi√ß√µes 5xx |

### Dashboard de SLO

O dashboard **Service Level Objectives (SLO) Dashboard** (`slo-dashboard.json`) mostra:

**Primeira linha (Stat panels):**
- Current SLI values (Availability, Latency P95, Error Rate)
- Cores: verde (atingindo SLO), amarelo (alerta), vermelho (violando SLO)

**Segunda linha (Gauges):**
- **Error Budget Remaining** (0-100%) - quanto "erro" ainda temos dispon√≠vel
- **Burn Rate** (1h) - velocidade de consumo do error budget

**Terceira linha (Time Series):**
- **Availability Trend** - hist√≥rico de disponibilidade com linha de threshold (99.9%)
- **Latency P95 Trend** - hist√≥rico de lat√™ncia com linha de threshold (200ms)

### Queries PromQL √öteis

```promql
# Availability da .NET API (√∫ltimos 5 minutos)
sum(rate(http_server_request_duration_seconds_count{job="dotnet-api",http_response_status_code=~"2.."}[5m]))
/
sum(rate(http_server_request_duration_seconds_count{job="dotnet-api"}[5m])) * 100

# Latency P95 da .NET API
histogram_quantile(0.95,
  sum(rate(http_server_request_duration_seconds_bucket{job="dotnet-api"}[5m])) by (le)
) * 1000

# Error Budget Remaining da .NET API (30 dias)
clamp_max(
  100 - (
    ((sum(rate(http_server_request_duration_seconds_count{job="dotnet-api",http_response_status_code=~"5.."}[30d])) or vector(0))
    / sum(rate(http_server_request_duration_seconds_count{job="dotnet-api"}[30d])) * 100)
    / 0.1
    * 100
  ),
  100
)

# Burn Rate da .NET API (1 hora)
(
  (sum(rate(http_server_request_duration_seconds_count{job="dotnet-api",http_response_status_code=~"5.."}[1h])) or vector(0))
  /
  sum(rate(http_server_request_duration_seconds_count{job="dotnet-api"}[1h]))
  * 100
) / 0.1
```

### Interpreta√ß√£o do Burn Rate

- **Burn Rate = 1.0**: Consumindo error budget na taxa esperada (OK)
- **Burn Rate = 5.0**: Consumindo 5x mais r√°pido (‚ö†Ô∏è ALERTA!)
- **Burn Rate = 10.0**: Consumindo 10x mais r√°pido (üö® CR√çTICO!)

**Exemplo:**
- SLO: 99.9% (error budget = 0.1% = ~43 minutos de downtime/m√™s)
- Se burn rate = 10x por 1 hora, em ~3 dias todo o budget mensal seria consumido
- A√ß√£o: Investigar imediatamente e resolver problemas antes de esgotar o budget

### Alertas Recomendados

**Alerta Critical - Burn Rate Alto:**
```yaml
alert: HighBurnRate
expr: |
  (
    sum(rate(http_server_request_duration_seconds_count{http_response_status_code=~"5.."}[1h]))
    /
    sum(rate(http_server_request_duration_seconds_count[1h]))
  ) > (0.001 * 10)  # 0.1% * 10 = 1%
for: 1h
labels:
  severity: critical
annotations:
  summary: "Error budget consumindo 10x mais r√°pido que o esperado"
```

**Alerta Warning - Error Budget Baixo:**
```yaml
alert: LowErrorBudget
expr: error_budget_remaining < 10
for: 5m
labels:
  severity: warning
annotations:
  summary: "Error budget < 10% restante"
```

### Documenta√ß√£o Completa

Ver `docs/slo.md` para documenta√ß√£o detalhada sobre:
- Conceitos de SLI/SLO/Error Budget/Burn Rate
- Queries PromQL para cada API
- Interpreta√ß√£o dos valores
- Refer√™ncias do Google SRE Book

---

## üèóÔ∏è Arquitetura do Projeto

### Estrutura de Diret√≥rios

```
lab-observabilidade/
‚îú‚îÄ‚îÄ apps/                                    # Aplica√ß√µes monitoradas
‚îÇ   ‚îú‚îÄ‚îÄ dotnet-api/                         # API .NET com OpenTelemetry
‚îÇ   ‚îú‚îÄ‚îÄ python-api/                         # API Python com OpenTelemetry
‚îÇ   ‚îú‚îÄ‚îÄ java-api/                           # API Java com Micrometer
‚îÇ   ‚îú‚îÄ‚îÄ nextjs-app/                         # App Next.js com prom-client
‚îÇ   ‚îú‚îÄ‚îÄ angular-app/                        # App Angular com Grafana Faro
‚îÇ   ‚îî‚îÄ‚îÄ nginx/                              # Nginx como reverse proxy
‚îÇ
‚îú‚îÄ‚îÄ observability/                          # Stack de observabilidade
‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prometheus.yml                  # Config de scrape targets
‚îÇ   ‚îú‚îÄ‚îÄ loki/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ loki-config.yml                # Config de armazenamento de logs
‚îÇ   ‚îú‚îÄ‚îÄ alloy/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ config.alloy                   # Config de coleta (logs + RUM)
‚îÇ   ‚îî‚îÄ‚îÄ grafana/
‚îÇ       ‚îî‚îÄ‚îÄ provisioning/                   # Configura√ß√£o autom√°tica
‚îÇ           ‚îú‚îÄ‚îÄ datasources/               # Prometheus + Loki
‚îÇ           ‚îî‚îÄ‚îÄ dashboards/                # Dashboards em JSON
‚îÇ               ‚îî‚îÄ‚îÄ json/
‚îÇ                   ‚îú‚îÄ‚îÄ apis-logs.json     # Logs consolidados
‚îÇ                   ‚îú‚îÄ‚îÄ linux.json         # Host Linux/WSL
‚îÇ                   ‚îú‚îÄ‚îÄ windows.json       # Host Windows + IIS
‚îÇ                   ‚îî‚îÄ‚îÄ [app]-*.json       # Dashboards por app
‚îÇ
‚îî‚îÄ‚îÄ docker-compose.yml                      # Orquestra√ß√£o completa
```

### Fluxo de Dados

**M√©tricas (Pull):**
1. Aplica√ß√µes exp√µem `/metrics` em formato Prometheus
2. Prometheus faz scrape a cada 15s (configurado em `prometheus.yml`)
3. Grafana consulta Prometheus via PromQL

**Logs (Push):**
1. Aplica√ß√µes geram logs no stdout/stderr
2. Alloy coleta via Docker logs API
3. Alloy adiciona labels (`container`, `job`) e envia para Loki
4. Grafana consulta Loki via LogQL

**RUM (Push):**
1. Angular app usa Grafana Faro SDK
2. SDK captura Core Web Vitals, erros, intera√ß√µes
3. Envia para Alloy via HTTP (porta 12347)
4. Alloy processa e envia para Loki

**Host Monitoring:**
- **Linux/WSL**: Node Exporter (porta 9100) ‚Üí Prometheus
- **Windows**: Windows Exporter (porta 9182) ‚Üí Prometheus

---

## üîß Componentes e Tecnologias

### Instrumenta√ß√£o por Linguagem

**API .NET (OpenTelemetry):**
- SDK: `OpenTelemetry.Extensions.Hosting`
- Exporters: `OpenTelemetry.Exporter.Prometheus.AspNetCore`
- M√©tricas autom√°ticas: HTTP, ASP.NET Core, Runtime (.NET GC)
- M√©tricas customizadas: `Meter` + `Counter`
- Endpoint: `/metrics`

**API Python (OpenTelemetry):**
- SDK: `opentelemetry-sdk`
- Instrumenta√ß√£o: `opentelemetry-instrumentation-fastapi`
- Exporter: `opentelemetry-exporter-prometheus`
- Endpoint: `/metrics`

**API Java (Micrometer):**
- SDK: `micrometer-registry-prometheus` (Spring Boot Actuator)
- M√©tricas autom√°ticas: HTTP, JVM, sistema
- Endpoint: `/actuator/prometheus`

**Next.js (prom-client):**
- Biblioteca: `prom-client`
- M√©tricas autom√°ticas: HTTP, Node.js runtime
- Endpoint: `/api/metrics`

**Angular (Grafana Faro):**
- SDK: `@grafana/faro-web-sdk`
- Captura: Core Web Vitals (LCP, FID, CLS), erros JS, navega√ß√£o
- Push para Alloy (porta 12347)

### Labels Importantes

**Loki:**
- `container`: nome do container Docker (ex: `dotnet-api`, `nginx`)
- `job`: identificador do servi√ßo (ex: `nginx`, `node-exporter-linux`)
- `type`: tipo de log (ex: `access`, `error`) - usado no Nginx

**Prometheus:**
- `job`: nome do job configurado em `prometheus.yml`
- `instance`: endere√ßo do target (ex: `dotnet-api:5000`)
- M√©tricas Windows: `job="node-exporter-windows"`
- M√©tricas Linux: `job="node-exporter-linux"`

---

## ‚ö†Ô∏è Problemas Conhecidos e Limita√ß√µes

### WSL2 e Filesystem 9p

O **Node Exporter Linux** n√£o consegue coletar m√©tricas de disco no WSL2 devido ao filesystem tipo 9p (Plan 9 Protocol).

**Solu√ß√£o aplicada:**
- Dashboard Linux (`linux.json`) tem painel "Disk Usage" substitu√≠do por mensagem informativa
- M√©tricas de disco do Windows devem ser consultadas no dashboard Windows

**Configura√ß√£o:**
```yaml
# docker-compose.yml - node-exporter-linux
command:
  - '--collector.filesystem.fs-types-exclude=^(autofs|...|9p)$$'
```

### Queries de CPU no WSL2

A fun√ß√£o `rate()` pode retornar valores incorretos no WSL2. Use `irate()` para queries de CPU:

```promql
# ‚úÖ Correto para WSL2
100 - (avg(irate(node_cpu_seconds_total{mode="idle",job="node-exporter-linux"}[5m])) * 100)

# ‚ùå Pode retornar valores > 100%
100 - (avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
```

### IIS Metrics

As m√©tricas do IIS no dashboard Windows s√≥ retornar√£o dados se o IIS estiver instalado e rodando no host Windows. Sem IIS, os pain√©is ficar√£o vazios (comportamento esperado).

### Logs vs M√©tricas

- **Loki** usa labels diferentes de Prometheus:
  - Loki: `{container="dotnet-api"}` ou `{job="nginx"}`
  - Prometheus: `{job="dotnet-api"}` ou `{instance="dotnet-api:5000"}`

---

## üìä Dashboards do Grafana

Todos os dashboards s√£o provisionados automaticamente em `observability/grafana/provisioning/dashboards/json/`:

| Dashboard | Arquivo | Descri√ß√£o |
|-----------|---------|-----------|
| APIs - Logs Consolidados | `apis-logs.json` | Logs de todas as APIs + Nginx |
| Multi-Language Overview | `multi-language-overview.json` | Vis√£o geral de todas as APIs |
| .NET API | `dotnet-api.json` | M√©tricas espec√≠ficas da API .NET |
| Python API | `python-api.json` | M√©tricas espec√≠ficas da API Python |
| Java API | `java-api.json` | M√©tricas espec√≠ficas da API Java |
| Next.js App | `nextjs-app.json` | M√©tricas espec√≠ficas do Next.js |
| Angular App | `angular-app.json` | RUM e Core Web Vitals |
| Nginx | `nginx.json` | M√©tricas do Nginx |
| PostgreSQL - Database Monitoring | `postgresql.json` | M√©tricas do PostgreSQL (Python API) |
| SQL Server - Database Monitoring | `mssql.json` | M√©tricas do SQL Server (.NET API) |
| MySQL - Database Monitoring | `mysql.json` | M√©tricas do MySQL (Java API) |
| WSL - Monitoramento do Sistema | `linux.json` | Monitoramento do WSL (Linux rodando no Windows) |
| HOST Windows + IIS | `windows.json` | Monitoramento do host Windows f√≠sico + IIS |

### Modificar Dashboards

**Op√ß√£o 1 - Via Grafana UI (tempor√°rio):**
1. Editar dashboard no Grafana
2. Exportar JSON (Share ‚Üí Export ‚Üí Save to file)
3. Copiar JSON para `observability/grafana/provisioning/dashboards/json/`
4. Reiniciar Grafana: `docker compose restart grafana`

**Op√ß√£o 2 - Editar JSON diretamente (permanente):**
1. Editar arquivo `.json` em `observability/grafana/provisioning/dashboards/json/`
2. Reiniciar Grafana: `docker compose restart grafana`

**IMPORTANTE:**
- Dashboards provisionados n√£o podem ser editados diretamente no Grafana UI
- Mudan√ßas feitas na UI s√£o perdidas ao reiniciar
- Sempre edite o JSON de origem

---

## üêõ Troubleshooting

### Container n√£o sobe

```bash
# Ver logs do container
docker logs <container-name>

# Verificar conflito de portas
docker compose ps
netstat -ano | findstr :<porta>  # Windows
lsof -i :<porta>  # Linux/macOS

# Rebuild for√ßado
docker compose down
docker compose up -d --build
```

### M√©tricas n√£o aparecem no Prometheus

```bash
# 1. Verificar se target est√° UP
curl http://localhost:9090/targets

# 2. Verificar se aplica√ß√£o est√° expondo m√©tricas
curl http://localhost:5000/metrics

# 3. Verificar configura√ß√£o do Prometheus
cat observability/prometheus/prometheus.yml

# 4. Restart do Prometheus
docker compose restart prometheus
```

### Logs n√£o aparecem no Loki

```bash
# 1. Verificar logs do Alloy
docker logs alloy

# 2. Testar query no Grafana Explore
{container="dotnet-api"}

# 3. Verificar se container est√° gerando logs
docker logs dotnet-api

# 4. Restart do Alloy
docker compose restart alloy
```

### Dashboard vazio no Grafana

1. Verificar se datasources est√£o configurados (Connections ‚Üí Data sources)
2. Verificar se Prometheus/Loki est√£o UP
3. Verificar queries no painel (Edit panel ‚Üí Query inspector)
4. Verificar se h√° dados no time range selecionado

---

## üîç Conven√ß√µes de C√≥digo

### Commits

Seguir [Conventional Commits](https://www.conventionalcommits.org/):
- `feat(api): adicionar endpoint de estat√≠sticas`
- `fix(docker): corrigir erro ao buildar Next.js`
- `docs(readme): adicionar troubleshooting`
- `refactor(metrics): extrair l√≥gica para service`

### Dashboards

**Naming convention:**
- Arquivos: `<nome>-<tipo>.json` (ex: `dotnet-api.json`, `apis-logs.json`)
- T√≠tulos: `[APP/SERVI√áO] - [Descri√ß√£o]` (ex: `.NET API - Dashboard`, `APIs - Logs Consolidados`)
- UIDs: `<nome>-<tipo>-monitoring` (ex: `dotnet-api-monitoring`, `host-monitoring`)

**Estrutura de pain√©is:**
1. Primeira linha: Status, gauges, stats (altura 7-8)
2. Linhas seguintes: Time series, gr√°ficos (altura 8)
3. Usar refresh: `10s` para dashboards de APIs
4. Usar time range: `now-30m` to `now` por padr√£o

### Docker

- Usar imagens Alpine quando poss√≠vel
- Multi-stage builds para otimiza√ß√£o
- `.dockerignore` em todos os projetos
- Health checks quando aplic√°vel
- Restart policy: `unless-stopped` para servi√ßos cr√≠ticos

---

## üìö Conceitos Importantes

### Os 3 Pilares da Observabilidade

1. **M√©tricas** - N√∫meros agregados (requisi√ß√µes/s, lat√™ncia, CPU)
2. **Logs** - Eventos textuais (erros, requisi√ß√µes HTTP, stack traces)
3. **Traces** - Caminho de requisi√ß√µes entre servi√ßos (n√£o implementado neste lab)

### Pull vs Push

- **Prometheus** = Pull-based (faz scrape das aplica√ß√µes)
- **Loki** = Push-based (Alloy envia logs para Loki)
- **Grafana Faro** = Push-based (SDK envia RUM para Alloy)

### PromQL B√°sico

```promql
# Counter - taxa de mudan√ßa
rate(metric_total[1m])

# Gauge - valor atual
metric_value

# Histogram - percentis
histogram_quantile(0.95, rate(metric_bucket[1m]))

# Agrega√ß√£o
sum by(job) (metric)
avg(metric)
max(metric) by (instance)
```

### LogQL B√°sico

```logql
# Filtro simples
{container="nginx"}

# M√∫ltiplos labels
{job="nginx", type="access"}

# Busca de texto
{container="dotnet-api"} |= "error"

# Taxa de logs
rate({container="python-api"}[5m])
```

---

<!-- BACKLOG.MD GUIDELINES START -->
# Instructions for the usage of Backlog.md CLI Tool

## Backlog.md: Comprehensive Project Management Tool via CLI

### Assistant Objective

Efficiently manage all project tasks, status, and documentation using the Backlog.md CLI, ensuring all project metadata
remains fully synchronized and up-to-date.

### Core Capabilities

- ‚úÖ **Task Management**: Create, edit, assign, prioritize, and track tasks with full metadata
- ‚úÖ **Search**: Fuzzy search across tasks, documents, and decisions with `backlog search`
- ‚úÖ **Acceptance Criteria**: Granular control with add/remove/check/uncheck by index
- ‚úÖ **Board Visualization**: Terminal-based Kanban board (`backlog board`) and web UI (`backlog browser`)
- ‚úÖ **Git Integration**: Automatic tracking of task states across branches
- ‚úÖ **Dependencies**: Task relationships and subtask hierarchies
- ‚úÖ **Documentation & Decisions**: Structured docs and architectural decision records
- ‚úÖ **Export & Reporting**: Generate markdown reports and board snapshots
- ‚úÖ **AI-Optimized**: `--plain` flag provides clean text output for AI processing

### Why This Matters to You (AI Agent)

1. **Comprehensive system** - Full project management capabilities through CLI
2. **The CLI is the interface** - All operations go through `backlog` commands
3. **Unified interaction model** - You can use CLI for both reading (`backlog task 1 --plain`) and writing (
   `backlog task edit 1`)
4. **Metadata stays synchronized** - The CLI handles all the complex relationships

### Key Understanding

- **Tasks** live in `backlog/tasks/` as `task-<id> - <title>.md` files
- **You interact via CLI only**: `backlog task create`, `backlog task edit`, etc.
- **Use `--plain` flag** for AI-friendly output when viewing/listing
- **Never bypass the CLI** - It handles Git, metadata, file naming, and relationships

---

# ‚ö†Ô∏è CRITICAL: NEVER EDIT TASK FILES DIRECTLY. Edit Only via CLI

**ALL task operations MUST use the Backlog.md CLI commands**

- ‚úÖ **DO**: Use `backlog task edit` and other CLI commands
- ‚úÖ **DO**: Use `backlog task create` to create new tasks
- ‚úÖ **DO**: Use `backlog task edit <id> --check-ac <index>` to mark acceptance criteria
- ‚ùå **DON'T**: Edit markdown files directly
- ‚ùå **DON'T**: Manually change checkboxes in files
- ‚ùå **DON'T**: Add or modify text in task files without using CLI

**Why?** Direct file editing breaks metadata synchronization, Git tracking, and task relationships.

---

## 1. Source of Truth & File Structure

### üìñ **UNDERSTANDING** (What you'll see when reading)

- Markdown task files live under **`backlog/tasks/`** (drafts under **`backlog/drafts/`**)
- Files are named: `task-<id> - <title>.md` (e.g., `task-42 - Add GraphQL resolver.md`)
- Project documentation is in **`backlog/docs/`**
- Project decisions are in **`backlog/decisions/`**

### üîß **ACTING** (How to change things)

- **All task operations MUST use the Backlog.md CLI tool**
- This ensures metadata is correctly updated and the project stays in sync
- **Always use `--plain` flag** when listing or viewing tasks for AI-friendly text output

---

## 2. Common Mistakes to Avoid

### ‚ùå **WRONG: Direct File Editing**

```markdown
# DON'T DO THIS:
1. Open backlog/tasks/task-7 - Feature.md in editor
2. Change "- [ ]" to "- [x]" manually
3. Add notes directly to the file
4. Save the file
```

### ‚úÖ **CORRECT: Using CLI Commands**

```bash
# DO THIS INSTEAD:
backlog task edit 7 --check-ac 1  # Mark AC #1 as complete
backlog task edit 7 --notes "Implementation complete"  # Add notes
backlog task edit 7 -s "In Progress" -a @agent-k  # Change status and assign
```

---

## 3. Understanding Task Format (Read-Only Reference)

‚ö†Ô∏è **FORMAT REFERENCE ONLY** - Never edit task files directly! Use CLI commands.

### How to Modify Each Section

| What You Want to Change | CLI Command to Use |
|-------------------------|-------------------|
| Title | `backlog task edit 42 -t "New Title"` |
| Status | `backlog task edit 42 -s "In Progress"` |
| Assignee | `backlog task edit 42 -a @sara` |
| Labels | `backlog task edit 42 -l backend,api` |
| Description | `backlog task edit 42 -d "New description"` |
| Add AC | `backlog task edit 42 --ac "New criterion"` |
| Check AC #1 | `backlog task edit 42 --check-ac 1` |
| Uncheck AC #2 | `backlog task edit 42 --uncheck-ac 2` |
| Remove AC #3 | `backlog task edit 42 --remove-ac 3` |
| Add Plan | `backlog task edit 42 --plan "1. Step one\n2. Step two"` |
| Add Notes | `backlog task edit 42 --notes "What I did"` |
| Append Notes | `backlog task edit 42 --append-notes "Another note"` |

---

## 4. Implementing Tasks Workflow

### Step 1: Start Work
```bash
# Assign yourself and set to In Progress
backlog task edit 42 -s "In Progress" -a @myself
```

### Step 2: Create Implementation Plan
```bash
# Think about HOW to solve the task
backlog task edit 42 --plan "1. Research\n2. Implement\n3. Test"
```

### Step 3: Share Plan with User
Share the plan and wait for approval before coding.

### Step 4: Implement
Write code, test, and mark acceptance criteria as complete:
```bash
backlog task edit 42 --check-ac 1 --check-ac 2
```

### Step 5: Add Implementation Notes
```bash
backlog task edit 42 --notes "Implemented using pattern X, modified files Y and Z"
```

### Step 6: Mark as Done
```bash
backlog task edit 42 -s Done
```

---

## 5. Quick Reference: Common Commands

```bash
# List tasks
backlog task list --plain
backlog task list -s "To Do" --plain

# View task
backlog task 42 --plain

# Search
backlog search "auth" --plain

# Create task
backlog task create "Task title" -d "Description" --ac "Criterion 1"

# Edit task
backlog task edit 42 -s "In Progress" -a @myself
backlog task edit 42 --check-ac 1
backlog task edit 42 --notes "Implementation complete"
```

---

## Remember: The Golden Rule

**üéØ If you want to change ANYTHING in a task, use `backlog task edit`**
**üìñ Use CLI to read tasks, exceptionally READ task files directly, never WRITE to them**

Full help: `backlog --help`

<!-- BACKLOG.MD GUIDELINES END -->
